# Uczenie ze Wzmocnieniem: Q-learning (Taxi-v3) i Deep Q-Network "from scratch" (LunarLander-v3)

**Autor:** Marcin Przybylski
**Data:** 2 maja 2025

## Opis Projektu

Niniejszy projekt dotyczy uczenia ze wzmocnieniem (Reinforcement Learning - RL). Celem byÅ‚o praktyczne zapoznanie siÄ™ z podstawowymi koncepcjami RL poprzez implementacjÄ™, trening i analizÄ™ dwÃ³ch popularnych algorytmÃ³w na klasycznych Å›rodowiskach z biblioteki Gymnasium (nastÄ™pcy OpenAI Gym).

Projekt zostaÅ‚ podzielony na dwie gÅ‚Ã³wne czÄ™Å›ci:
1.  **RozwiÄ…zanie Å›rodowiska Taxi-v3 przy uÅ¼yciu algorytmu Q-learning:** Åšrodowisko to charakteryzuje siÄ™ dyskretnÄ… przestrzeniÄ… stanÃ³w i akcji. Agent (taksÃ³wka) uczyÅ‚ siÄ™ optymalnej strategii odbierania i dostarczania pasaÅ¼era.
2.  **RozwiÄ…zanie Å›rodowiska LunarLander-v3 przy uÅ¼yciu algorytmu Deep Q-Network (DQN) implementowanego "from scratch":** Åšrodowisko lÄ…dowania na KsiÄ™Å¼ycu posiada ciÄ…gÅ‚Ä… przestrzeÅ„ stanÃ³w. Zaimplementowano algorytm DQN, w ktÃ³rym funkcja wartoÅ›ci Q jest aproksymowana przez sieÄ‡ neuronowÄ…. Implementacja zostaÅ‚a wykonana od podstaw przy uÅ¼yciu biblioteki NumPy, aby lepiej zrozumieÄ‡ wewnÄ™trzne mechanizmy algorytmu, w tym pamiÄ™Ä‡ powtÃ³rek (Experience Replay) i sieÄ‡ docelowÄ… (Target Network).

## ğŸ“‚ Zasoby Projektu

Prace zostaÅ‚y zrealizowane w Å›rodowisku Google Colaboratory.

## âš™ï¸ Metodologia

### 1. Przygotowanie Åšrodowisk

* **Taxi-v3:**
    * Dyskretna przestrzeÅ„ stanÃ³w: 500.
    * Dyskretna przestrzeÅ„ akcji: 6 (ruchy w 4 kierunkach, podniesienie, wysadzenie pasaÅ¼era).
    * Render mode: `rgb_array`.
* **LunarLander-v3:**
    * CiÄ…gÅ‚a, 8-wymiarowa przestrzeÅ„ stanÃ³w (pozycja x, y, prÄ™dkoÅ›Ä‡ x, y, kÄ…t, prÄ™dkoÅ›Ä‡ kÄ…towa, kontakt nogi lewej/prawej).
    * Dyskretna przestrzeÅ„ akcji: 4 (nic, odpalenie lewego silnika, gÅ‚Ã³wnego, prawego).
    * Render mode: `rgb_array`.

Dla obu Å›rodowisk przeprowadzono wstÄ™pnÄ… eksploracjÄ™ i sprawdzono dziaÅ‚anie agenta wykonujÄ…cego losowe akcje.

### 2. Implementacja AlgorytmÃ³w

#### 2.2.1 Q-learning (dla Taxi-v3)

* WartoÅ›ci Q przechowywane w tablicy NumPy (500, 6), inicjalizowanej zerami.
* Strategia eksploracji: e-greedy, gdzie $\epsilon$ liniowo zanikaÅ‚o od 1.0 do 0.01.
* Aktualizacja wartoÅ›ci Q: $Q(S,A) \leftarrow Q(S,A) + \alpha[R + \gamma \max_{a'}Q(S',a') - Q(S,A)]$.

#### 2.2.2 Deep Q-Network (DQN) "from scratch" (dla LunarLander-v3)

* **SieÄ‡ neuronowa (NumPy):**
    * Warstwa wejÅ›ciowa: 8 neuronÃ³w.
    * Dwie warstwy ukryte z aktywacjÄ… ReLU: 256 i 128 neuronÃ³w.
    * Warstwa wyjÅ›ciowa: liniowa, 4 neurony (po jednym dla kaÅ¼dej akcji).
    * Inicjalizacja wag: He (dla ReLU), Glorot/Xavier (dla liniowej).
* **PamiÄ™Ä‡ powtÃ³rek (Experience Replay):** Bufor `deque` o rozmiarze 100 000, przechowujÄ…cy krotki (S, A, R, S', done).
* **SieÄ‡ docelowa (Target Network):** Osobna kopia sieci gÅ‚Ã³wnej, wagi synchronizowane co 1000 krokÃ³w.
* **Aktualizacja (Trening):** Co 4 kroki losowano minibatch (rozmiar 64) z pamiÄ™ci. Obliczano docelowe wartoÅ›ci Q (y) i wartoÅ›ci Q przewidziane przez sieÄ‡ gÅ‚Ã³wnÄ…. BÅ‚Ä…d ($Q_{current} - Q_{target\_bp}$ z clippingiem do [-1, 1]) propagowany wstecznie.
* Strategia eksploracji: e-greedy z liniowym zanikiem $\epsilon$.
* Dodatkowo: zanik wspÃ³Å‚czynnika uczenia (learning rate).

### 2.3 Proces Treningu

* **Q-learning (Taxi-v3):**
    * Liczba epizodÃ³w: 15 000.
    * Parametry: $\alpha=0.1$, $\gamma=0.9$, $\epsilon$ zanikajÄ…ce od 1.0 do 0.01 przez 80% epizodÃ³w.
* **DQN (LunarLander-v3):**
    * Liczba epizodÃ³w: 2 000.
    * Parametry: $\gamma=0.99$, rozmiar batcha = 64, $\epsilon$ zanikajÄ…ce od 1.0 do 0.01 przez â‰ˆ 600 000 krokÃ³w, LR zanikajÄ…ce od $5 \times 10^{-4}$ do $1 \times 10^{-5}$ przez â‰ˆ 600 000 krokÃ³w.
    * SieÄ‡ docelowa aktualizowana co 1000 krokÃ³w, trening sieci gÅ‚Ã³wnej co 4 kroki (po zebraniu 640 prÃ³bek).

Monitorowano sumÄ™ nagrÃ³d w kaÅ¼dym epizodzie oraz (dla DQN) Å›redni loss.

### 2.4 Ewaluacja Modeli

* **Metryki iloÅ›ciowe:** Åšrednia nagroda w ostatnich 100 epizodach treningu. Dedykowana faza ewaluacji (kilka epizodÃ³w bez eksploracji $\epsilon$-greedy) z obliczeniem Å›redniej nagrody, Å›redniej liczby krokÃ³w i odsetka pomyÅ›lnie zakoÅ„czonych epizodÃ³w.
* **Wizualizacje:** Wykresy Å›redniej kroczÄ…cej nagrody (i loss dla DQN). Animacje GIF pokazujÄ…ce dziaÅ‚anie nauczonych agentÃ³w.

## ğŸ› ï¸ Wykorzystane NarzÄ™dzia i Technologie

* Python
* Gymnasium
* NumPy
* Matplotlib
* PIL (Pillow)
* imageio

## ğŸ“Š Wyniki i Interpretacja

### 3.1 Q-learning (Taxi-v3)

* **Wyniki treningu:** Po 15 000 epizodÃ³w, Å›rednia nagroda w ostatnich 100 epizodach ustabilizowaÅ‚a siÄ™ na poziomie **7.12**. Trening zajÄ…Å‚ okoÅ‚o 37 sekund.
* **Wykres uczenia:** WyraÅºny wzrost Å›redniej nagrody kroczÄ…cej, wskazujÄ…cy na efektywnÄ… naukÄ™.
* **Wyniki ewaluacji (3 epizody bez eksploracji):**
    * Åšrednia nagroda: **10.33**.
    * Odsetek sukcesu: **100%**.
    * Åšrednia liczba krokÃ³w: **10.67**.
* **Interpretacja:** Algorytm Q-learning doskonale poradziÅ‚ sobie ze Å›rodowiskiem Taxi-v3. Dyskretna i stosunkowo niewielka przestrzeÅ„ stanÃ³w pozwoliÅ‚a na efektywne wypeÅ‚nienie tablicy Q i znalezienie optymalnej polityki.

### 3.2 Deep Q-Network (LunarLander-v3)

* **Wyniki treningu:** Po 2 000 epizodÃ³w (ok. 209 tys. krokÃ³w), Å›rednia nagroda w ostatnich 100 epizodach wyniosÅ‚a **-68.32**. Trening trwaÅ‚ okoÅ‚o 578 sekund (prawie 10 minut).
* **Wykresy uczenia:** Tendencja wzrostowa Å›redniej nagrody kroczÄ…cej (choÄ‡ pozostaje ujemna) oraz tendencja spadkowa Å›redniego lossu, co wskazuje na proces uczenia.
* **Wyniki ewaluacji (3 epizody bez eksploracji):**
    * Åšrednia nagroda: **-132.40**.
    * Odsetek sukcesu (prÃ³g >190 pkt): **0%**.
* **Interpretacja:** Implementacja DQN "from scratch" dziaÅ‚aÅ‚a poprawnie i agent wykazywaÅ‚ postÄ™py. Jednak Å›rodowisko LunarLander jest znacznie bardziej zÅ‚oÅ¼one. OsiÄ…gniÄ™cie dobrych wynikÃ³w (Å›rednia nagroda > 200) wymaga zazwyczaj znacznie dÅ‚uÅ¼szego treningu i potencjalnie dalszego strojenia hiperparametrÃ³w. Wynik -68.32 po 2000 epizodÃ³w jest typowy dla poczÄ…tkowej fazy uczenia.

## ğŸ Podsumowanie i Wnioski

1.  **SkutecznoÅ›Ä‡ Q-learningu w Å›rodowiskach dyskretnych:** Q-learning okazaÅ‚ siÄ™ bardzo efektywny i szybko zbieÅ¼ny w Å›rodowisku Taxi-v3.
2.  **KoniecznoÅ›Ä‡ i zÅ‚oÅ¼onoÅ›Ä‡ DQN dla Å›rodowisk ciÄ…gÅ‚ych:** Dla LunarLander-v3 konieczne byÅ‚o zastosowanie DQN. OsiÄ…gniÄ™cie wysokiej wydajnoÅ›ci wymaga znacznie wiÄ™cej czasu treningu i optymalizacji.
3.  **Wyzwania implementacji "from scratch":** Implementacja DQN od podstaw przy uÅ¼yciu NumPy jest pouczajÄ…ca, ale bardziej zÅ‚oÅ¼ona i podatna na bÅ‚Ä™dy niÅ¼ korzystanie z gotowych bibliotek RL.
4.  **Znaczenie hiperparametrÃ³w i czasu treningu:** Wyniki dla DQN podkreÅ›lajÄ… kluczowÄ… rolÄ™ odpowiedniego doboru hiperparametrÃ³w oraz wystarczajÄ…co dÅ‚ugiego czasu treningu.
5.  **Praktyczne aspekty pracy z Gymnasium:** Projekt wymagaÅ‚ dostosowania do API biblioteki `gymnasium` oraz obsÅ‚ugi zaleÅ¼noÅ›ci i konfiguracji Å›rodowiska.

---
